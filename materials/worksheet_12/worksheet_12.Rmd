---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.12.0
  kernelspec:
    display_name: R
    language: R
    name: ir
---

<!-- #region nbgrader={"grade": false, "grade_id": "cell-f1e1d845873036f4", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
# Worksheet 11: Introduction to Explanatory Modeling when the Response Variable is Categorical or Discrete Counts
<!-- #endregion -->

<!-- #region nbgrader={"grade": false, "grade_id": "cell-82d9926086d47a80", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
#### Lecture and Tutorial Learning Goals:
After completing this week's lecture and tutorial work, you will be able to:

1. Describe the logistic regression estimation procedure (categorical data as the response variable and explanatory variables), and Poisson regression estimation procedure (discrete counts as the response variable and explanatory variables).
2. Discuss the relationship between linear regression and logistic and Poisson regression. Discuss the consequences of modeling data that is more suitable for logistic and Poisson regression models as a linear regression model.
3. Interpret the coefficients and $p$-values in the logistic and Poisson regression settings.
4. Discuss useful diagnostics for logistic and Poisson regression and explain why they should be performed.
5. Write a computer script to perform logistic and Poisson regression and perform model diagnostics. Interpret and communicate the results from that computer script.
<!-- #endregion -->

```{r nbgrader={'grade': False, 'grade_id': 'cell-a2a153352bc44a68', 'locked': True, 'schema_version': 3, 'solution': False, 'task': False}}
# Run this cell before continuing.
library(tidyverse)
library(repr)
library(digest)
library(infer)
library(gridExtra)
library(mlbench)
library(AER)
library(ISLR)
library(broom)
library(qqplotr)
library(performance)
library(see)
library(MASS)
library(glmbb)
library(cowplot)
source("tests_worksheet_12.R")
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-82e529f0a9216400"} -->
## 1. Intro

In previous week, you explored the Multiple Linear Regression (MLR) as a way to model the mean of a numeric response variable, $Y$, given a set of covariate $\mathbf{X}$:

$$
E\left[Y\left|\mathbf{X}=\left(X_1,...,X_p\right)\right.\right] = \beta_0 + \beta_1X_1 + \ldots + \beta_pX_p
$$

However, in some situations, the MLR is not suitable. This week we are going to study two of those situations that commonly arises in practice:

- the case of dichotomous variables (e.g., yes/no, success/failure, win/lose, sick/not sick); 
- the case of counts (e.g., number of cases of a rare disease in Vancouver in a period of one year; the number of accidents in the Canada Highway in a period of one month;)
<!-- #endregion -->

<!-- #region nbgrader={"grade": false, "grade_id": "cell-9be71f65643c5906", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
## 1. Logistic Regression


<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-7aad649d27ed77eb"} -->
The first section of this worksheet will focus on the dataset `Default` from [*An Introduction to Statistical Learning*](https://www.statlearning.com/) (James et al., 2013). This is a dataset of $n = 10,000$ observations with the following variables:

- `default`: a binary response indicating whether the customer defaulted on their debt (`Yes` or `No`).
- `student`: a binary input variable indicating whether the customer is a student (`Yes` or `No`).
- `balance`: a continuous input variable indicating the remaining average balance of the customer's credit card (after the monthly payment).
- `income`: a continuous input variable indicating the customer's income.
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': False, 'locked': True, 'task': False, 'grade_id': 'cell-be65b9244ff2e164'}}
head(Default)
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-435c6ac14d8ae3f6"} -->
**Question 1.0**
<br>{points: 1}

We want to assess whether the response `default` is statistically associated to the input variables `student`, `balance`, and `income`. We have to set our binary response $Y_i$ mathematically as:

$$
Y_i =
\begin{cases}
1 \; \; \; \; \mbox{if the $i$th customer is in default},\\
0 \; \; \; \; 	\mbox{otherwise.}
\end{cases}
$$

Note that the "$1$" category is referred as *success*. Moreover, each $Y_i$ is a Bernoulli trial whose probability of success is $p_i$, i.e., 

$$Y_i \sim \text{Bernoulli}(p_i).$$

In the  `default` column, replace the levels `Yes` and `No` with the numerical values `1` and `0`, respectively.
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-f583c10048ebd9d7'}}
# Default <- 
#     Default %>% 
#     ...(... = ...(..., 1, 0))

### BEGIN SOLUTION
Default <- 
    Default %>% 
    mutate(default = if_else(default == "Yes", 1, 0))
### END SOLUTION

head(Default)
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-80e35ff00a0d8926'}}
test_1.0()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-ec0e34aef0968060"} -->
**Question 1.1**
<br>{points: 1}

Firstly, suppose we use the `1`s and `0`s in the response `default` as probabilities and we estimate a ordinary simple linear regression (SLR) to predict the mean of $Y_i$ (i.e., $p_i)$ subject to the continuous input `balance`:

$$
\mathbb{E}(Y_i \mid X_{\textit{i,balance}}) = p_i = \beta_0 + \beta_1 X_{\textit{i,balance}}.
$$

Create a plot of the data (using `geom_point()`) along with the estimated regression line (using `geom_smooth()` with `method = "lm"`). Include proper axis labels. The `ggplot()` object's name will be `Default_SLR_plot`.

*Fill out those parts indicated with `...`, uncomment the corresponding code in the cell below, and run it.*
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-beeb841f20cbe0b7'}}
options(repr.plot.width = 15, repr.plot.height = 7) # Adjust these numbers so the plot looks good in your desktop.

# Default_SLR_plot <- ...(...) +
#   ...(aes(..., ...)) +
#   ...(aes(..., ...), method = ..., se = FALSE) +
#   labs(y = ..., x = ...) +
#   ggtitle(...) +
#   theme(
#     text = element_text(size = 16.5),
#     plot.title = element_text(face = "bold"),
#     axis.title = element_text(face = "bold"),
#     legend.title = element_text(face = "bold")) +
#   scale_x_continuous(breaks = seq(0, 2500, 500))


### BEGIN SOLUTION
Default_SLR_plot <- ggplot(Default) +
  geom_point(aes(balance, default)) +
  geom_smooth(aes(balance, default), method = "lm", se = FALSE) +
  labs(y = "Prob. of Default", x = "Balance") +
  ggtitle("Simple Linear Regression") +
  theme(
    text = element_text(size = 16.5),
    plot.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold"),
    legend.title = element_text(face = "bold"),
  ) +
  scale_x_continuous(breaks = seq(0, 2500, 500))
### END SOLUTION

Default_SLR_plot
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-5f01635dd4cf6b6b'}}
test_1.1()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-a0c60c4720d6d478"} -->
**Class Discussion:** 

Do you see any problems with our model?  
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-33c529c35aa6d29d"} -->
**Question 1.2**
<br>{points: 1}

To overcome the restrticted range problem, instead of using the linear model to estimate $p_i$ directly, i.e.,  

$$\mathbb{E}(Y_i|X_{\textit{i,balance}}) = p_i = \beta_0 + \beta_1X_{i,\textit{balance}},$$ 

we could use a curve that is always between $[0,1]$. One of such curves is the logistic curve:

$$\mathbb{E}(Y_i|X_{\textit{i,balance}}) = p_i = \frac{\exp{\left\{\beta_0 + \beta_1X_{\textit{i,balance}}\right\}}}{1+\exp{\left\{\beta_0 + \beta_1X_{\textit{i,balance}}\right\}}}$$ 

Let's see how the logistic curve looks like. In this exercise, you are going to plot the logistic curve to see how it behaves.

_Save the plot in an object named `logistic_curve`._
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-ebd9f096f835b1b0'}}
# logistic_curve <-
#     tibble(z = seq(-10,10,0.01),
#            logistic_z = ...) %>% 
#     ggplot(aes(z, ...)) + 
#     geom_line() +
#     geom_hline(yintercept = 1, lty=2) + 
#     geom_hline(yintercept = 0, lty=2) +
#     theme(text = element_text(size = 20)) + 
#     ggtitle("Logistic curve")

### BEGIN SOLUTION
logistic_curve <-
    tibble(z = seq(-10,10,0.01),
           logistic_z = exp(z)/(1+exp(z))) %>% 
    ggplot(aes(z, logistic_z)) + 
    geom_line() +
    geom_hline(yintercept = 1, lty=2) + 
    geom_hline(yintercept = 0, lty=2) +
    theme(text = element_text(size = 20)) +
    ggtitle("Logistic curve")
### END SOLUTION

logistic_curve
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-7728e71dc711badb'}}
test_1.2()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-1f6a141622c13896"} -->
**Question 1.3: Understanding the odds**
<br>{points: 1}

By using the logistic curve to model the probability of the response given the covarites, in this case, 

$$\mathbb{E}(Y_i|X_{\texttt{balance}_i}) = p_i = \frac{\exp{\left\{\beta_0 + \beta_1X_{\texttt{balance}_i}\right\}}}{1+\exp{\left\{\beta_0 + \beta_1X_{\texttt{balance}_i}\right\}}}$$ 

instead of using $\beta_0 + \beta_1X_{\texttt{balance}_i}$ to predict $p_i$ directly, we are using it to predict 

\begin{equation*}
\log\left(\frac{p_i}{1 - p_i}\right) = \beta_0 + \beta_1 X_{\texttt{balance}_i}.
\end{equation*}

The function $\log\left(\frac{p_i}{1 - p_i}\right)$ is called `logit`, and it is *logarithm of the odds*. Answer the question below:

> Vancouver Canucks is playing against Calgary Flames in the Final of NHL. The match will be at Rogers' arena, Canucks home. It is expected that out of 18,910 seats in the arena, 13700 seats will be occupied by Canucks fans. During the match, prizes are randomly distributed among the seats. What are the odds that a Canucks fan wins a given prize? 

Assign your answer to an object named `answer1.3`.
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-ee8b9ad3125f8346'}}
#answer1.3 <- ...

### BEGIN SOLUTION
answer1.3 <- 13700/(18910-13700)
### END SOLUTION

answer1.3
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-85f66b5726146ab1'}}
test_1.3()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-8a7c7401c453d69c"} -->
**Question 1.4:**
<br>{points: 1}

Let us plot the predictions of the binary logistic regression model on top of `Default_SLR_plot`. Use `geom_smooth()` with `method = "glm"` and `method.args = c(family = binomial)`.

*Fill out those parts indicated with `...`, uncomment the corresponding code in the cell below, and run it.*
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-47acd27ded854e48'}}
# Default_SLR_plot <- 
#     Default_SLR_plot +
#     ...(aes(..., ...),
#         ...,
#         ..., se = FALSE, color = "red") +
#     ggtitle("Simple Linear Regression and Binary Logistic Regression")

### BEGIN SOLUTION
Default_SLR_plot <- 
    Default_SLR_plot +
    geom_smooth(aes(balance, default),
                method = "glm",
                method.args = c(family = binomial), se = FALSE, color = "red") +
  ggtitle("Simple Linear Regression and Binary Logistic Regression")
### END SOLUTION

Default_SLR_plot
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-4989d973853e512d'}}
test_1.4()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-a628cff07e7aa9ce"} -->
Much better, isn't? 

--------------------------
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-ed0a86b15f54a046"} -->
Ok, let's review our model!

In this case, we have that 

$$
Y_i =
\begin{cases}
1 \; \; \; \; \mbox{if the $i$th customer is in default},\\
0 \; \; \; \; 	\mbox{otherwise.}
\end{cases}
$$

As the response variable can only take the values $0$ or $1$, the key parameter becomes the probability that $Y_i$ takes on the value of $1$, i.e. the probability that the customer is in default, denoted as $p_i$. Hence:

$$\left.Y_i\right|X_{i, \textit{student}}, X_{i, \textit{balance}}, X_{i, \textit{income}} \sim \text{Bernoulli}(p_i).$$

The binary logistic regression models the probability $p_i$ given some contextual information (i.e., our covariates). To re-express $p_i$ on an unrestricted scale, the modelling is done in terms of the logit function:

$$
\mbox{logit}(p_i) = \log \bigg( \frac{p_i}{1 - p_i}\bigg) = \beta_0 + \beta_1 X_{i, \textit{student}} + \beta_2 X_{i, \textit{balance}} + \beta_{3} X_{i, \textit{income}},
$$

or equivalently

$$
p_i = \frac{\exp\big[\beta_0 + \beta_1 X_{i, \textit{student}} + \beta_2 X_{i, \textit{balance}} + \beta_{3} X_{i, \textit{income}}\big]}{1 + \exp\big[\beta_0 + \beta_1 X_{i, \textit{student}} + \beta_2 X_{i, \textit{balance}} + \beta_{3} X_{i, \textit{income}}\big]}.
$$

The response in this GLM is called the log-odds, the logarithm of the odds $p_i/(1 - p_i)$, the ratio of the probability of the event to the probability of the non-event. For instance, if the event is that the customer is in the default, it denotes how likely the $i$th customer is to be in default compared to how unlikely it is. 


The question now is, how do we estimate the coefficients $\beta_0, \beta_1, \beta_2,$ and, $\beta_3$? So far, in the case of linear regression, we have been using the Least Square Estimators. However, we won't be using least square this week. We are going to use a method called Maximum Likelihood Estimators (MLE). Details of MLE are outside the scope of this course. Luckily for us, R can assist us with that! 
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-148b1dc1b2667581"} -->
**Question 1.5: Estimation**
<br>{points: 1}

In order to fit the model, we can use the function `glm()` and its argument `family = binomial` (required to specify the binary nature of the response). Let us use the function `glm()` to estimate a binary logistic regression. Using `Default`, we will fit a binary logistic model with `default` as the response and `student`, `balance`, and `income` as input variables.
    
Store the model in an object named `Default_binary_log_model`. The `glm()` parameters are analogous to `lm()` (`formula` and `data`) with the addition of `family = binomial` for this specific model. 
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-7097573be6a38bf2'}}
# Default_binary_log_model <- 
#   ...(
#        ...,
#        ...,
#        ...)


### BEGIN SOLUTION
Default_binary_log_model <- 
    glm(
        formula = default ~ student + balance + income,
        data = Default,
        family = binomial)
### END SOLUTION

summary(Default_binary_log_model)
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-a0e923b260f01656'}}
test_1.5()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-8357d4c87f849b10"} -->


--------------------------

We could make inference with our binary logistic regression model, i.e., we can determine whether an input variable is statistically associated with the logarithm of the odds through hypothesis testing for the parameters $\beta_j$. Hence, we will need information about the estimated regression coefficient $\hat{\beta}_j$ and its corresponding variability, which is reflected in the standard error of the estimate, $\mbox{SE}\left(\hat{\beta}_j\right)$. 

To determine the statistical significance of the regression coefficient, you can use the Wald statistic $z_j$

$$z_j = \frac{\hat{\beta}_j}{\mbox{SE}\left(\hat{\beta}_j\right)}$$

to test the hypotheses
\begin{gather*}
H_0: \beta_j = 0\\
H_a: \beta_j \neq 0.
\end{gather*}

A statistic like $z_j$ is referred to as a $t$-value in ordinary linear regression. However, in binary logistic regression, provided the sample size $n$ is large enough, $z_j$ has an approximately standard normal distribution under $H_0$ rather than a $t$-distribution.

Furthermore, given a specified level of confidence, we can construct approximate $(1 - \alpha) \times 100\%$ confidence intervals for the corresponding true value of $\beta_j$:

$$\hat{\beta}_j \pm z_{\alpha/2}\mbox{SE}\left(\hat{\beta}_j\right),$$

where $z_{\alpha/2}$ is the upper $\alpha/2$ quantile of the standard normal distribution.
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-c18026a5fd84fca7"} -->
**Question 1.6: Inference**
<br>{points: 1}

Report the estimated coefficients, their standard errors, and corresponding $p$-values by calling `tidy()` on `Default_binary_log_model`. Include the corresponding asymptotic 95% confidence intervals. 

_Store the results in the variable `Default_binary_log_model_results`._
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-ad37611aa165ca7a'}}
# Default_binary_log_model_results <- 
#   ...(..., conf.int = TRUE) 

### BEGIN SOLUTION
Default_binary_log_model_results <- 
    tidy(Default_binary_log_model, conf.int = TRUE) 
### END SOLUTION

Default_binary_log_model_results
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-38f7c2eea95906c9'}}
test_1.6()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-82749136a082e937"} -->
**Question 1.7: Inference**
<br>{points: 1}

Add the estimated effect each of the variables has on the **odds** to the `Default_binary_log_model_results` tibble. Make sure to also include the confidence interval for these effects.
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-366329ae6e91a284'}}
# Default_binary_log_model_results <- 
#   ... %>%
#   mutate(
#     exp.estimate = ...,
#     exp.conf.low = ...,
#     exp.conf.high = ...) %>%
#   mutate_if(is.numeric, round, 6)

### BEGIN SOLUTION
Default_binary_log_model_results <- 
    Default_binary_log_model_results %>%
    mutate(
        exp.estimate = exp(estimate),
        exp.conf.low = exp(conf.low),
        exp.conf.high = exp(conf.high)) %>%
    mutate_if(is.numeric, round, 6)
### END SOLUTION

Default_binary_log_model_results
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-b905bb95b0e06997'}}
test_1.7()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-d9141f6447689b01"} -->
**Question 1.8**
<br>{points: 1}

Using a **significance level $\alpha = 0.05$**, which inputs are statistically associated to the response via their regression coefficients in `Default_binary_log_model_results`?

**A.** The categorical input `student`.

**B.** The continuous input `balance`.

**C.** The continuous input `income`.

*Assign your answers to the object `answer1.8`. Your answers have to be included in a single string indicating the correct options **in alphabetical order** and surrounded by quotes (e.g., `"ABC"` indicates you are selecting the three options).*
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-822cb96c53313b4e'}}
# answer1.8 <- 

### BEGIN SOLUTION
answer1.8 <- "AB"
### END SOLUTION
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-a04e784c42d742dc'}}
test_1.8()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-5fe352592ed23811"} -->
**Question 1.9**
<br>{points: 1}

Considering the `Default_binary_log_model_results` tibble, what is the correct interpretation of the  $\hat{\beta}_\textit{student}$?

**A.** A customer who is *not* a student is $1 / 0.524 = 1.908$ times more likely to be in default than not to, while keeping the rest of the input variables constant.

**B.** A customer who is *not* a student is $1 / 0.647 = 1.546$ times more likely to be in default than not to, while keeping the rest of the input variables constant.

**C.** A customer who is a student is $1 / 0.524 = 1.908$ times more likely to not be in default than being in default, while keeping the rest of the input variables constant.

**D.** A customer who is a student is $1 / 0.524 = 1.908$ times more likely to be in default than not to, while keeping the rest of the input variables constant.

*Assign your answer to the object `answer1.9` (character type surrounded by quotes).*
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-11677807d101b133'}}
# answer1.9 <- ...

### BEGIN SOLUTION
answer1.9 <- "C"
### END SOLUTION

answer1.9
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-30f034478ea08799'}}
test_1.9()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-7960f6f613499491"} -->
**Question 1.10**
<br>{points: 1}

What is the correct interpretation of the regression equation's estimated slope for `balance`?

**A.** For each \$1 increase in `balance`, a customer is $1.006$ times more likely to be in default that not to while keeping the rest of the input variables constant. 

**B.** For each \$1 increase in `balance`, a customer is $1 / 1.006 = 0.994$ times more likely to be in default that not to while keeping the rest of the input variables constant. 

**C.** For each \$1 increase in `balance`, a customer is $1 / 1.006 = 0.994$ times more likely to be in default that not to.

**D.** For each \$1 increase in `balance`, a customer is $1.006$ times more likely to be in default that not to.

*Assign your answer to the object `answer1.10` (character type surrounded by quotes).*
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-7441e6aa1307023f'}}
# answer1.10 <- ...

### BEGIN SOLUTION
answer1.10 <- "A"
### END SOLUTION

```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-ca20c96b683b2072'}}
test_1.10()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-cfbfd19ea63f8152"} -->
**Question 1.11**
<br>{points: 1}

Besides inference, `Default_binary_log_model` can also be used to obtain predictions. Suppose we want to predict the odds of a customer being in default to not to. This customer is a `student` who has a credit card `balance` of `2200` with an income of `35000`.

Mathematically, our predicted log odds will look as 

\begin{gather*} 
\log \bigg( \frac{\hat{\pi}}{1 - \hat{\pi}}\bigg) = \underbrace{-10.869045}_{\hat{\beta}_0} - \underbrace{0.646776}_{\hat{\beta}_1} \times (1) + \underbrace{0.005737}_{\hat{\beta}_2} \times (2200) + \underbrace{0.000003}_{\hat{\beta}_2} \times (35000)= 1.21 \\
\end{gather*}

Next, by taking the exponential on both sides of the equation, we obtain our predicted *odds*: 

$$
\frac{\hat{p}_i}{1 - \hat{p}_i} = e^{1.21} = 3.36.
$$

Finally, solving the above for $\hat{p}_i$, we obtain our predicted probability of default

$$
\hat{p}_i = 3.36/4.36 = 0.7706
$$

Using `predict` and `Default_binary_log_model`, obtain the odds prediction above.

> **Hint:** Check the argument `type` when coding this prediction.

*Assign your answer to the object `answer1.11`. Fill out those parts indicated with `...`, uncomment the corresponding code in the cell below, and run it.*
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-23f0d706c79c9f1c'}}
# answer1.11 <- exp(...(...,
#   tibble(..., ..., ...),
#   type = ...))

### BEGIN SOLUTION
answer1.11 <- exp(predict(Default_binary_log_model,
  tibble(student = "Yes", balance = 2200, income = 35000),
  type = "link"))
### END SOLUTION

answer1.11
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-e8a22ef2e4ac8fe6'}}
test_1.11()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-f84da54cd4da6b01"} -->
**Question 1.12**
<br>{points: 1}

We can also predict probabilities for classification purposes, i.e., whether the customer will default or not. Using the function `predict()` with the object `Default_binary_log_model`, obtain the estimated probability for a customer being in default. This customer is a `student` who has a credit card `balance` of `2200` with an income of `35000`.

> **Hint:** Check the argument `type` when coding this prediction.

*Assign your answer to the object `answer1.12`. Fill out those parts indicated with `...`, uncomment the corresponding code in the cell below, and run it.*
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-ee0c66cf674510cc'}}
# answer1.12 <- 
#   ...(...,
#     tibble(..., ..., ...),
#     type = ...)

### BEGIN SOLUTION
answer1.12 <- 
    predict(Default_binary_log_model,
        tibble(student = "Yes", balance = 2200, income = 35000),
        type = "response")
### END SOLUTION

answer1.12
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-03c8e23b29226318'}}
test_1.12()
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-5d4b54303734b4c1"} -->
**Overdispersion**

The variance of the Binomial model is a function of the mean: $np(1-p)$. What this means is that, your estimate of the mean also specifies your variance. So, our model assumes that, for whatever $\hat{p}$ you are estimating, the model is assuming a variability of $n\hat{p}(1-\hat{p})$. Note that in our case here, $n=1$. 

Unfortunately, even in situation where the model seems to be estimating the mean well, the variability of the data is not quite compatible with the model's assumed variance. What we can do is to consider a dispersion parameter, usually called $\phi$, which will help us correct the standard error of our estimators. 

The correction, is quite simple, we just multiply the standard error by the dispersion parameter. Let's see an example.  
<!-- #endregion -->

```{r}
summary(glm(
    formula = default ~ student + balance + income,
    data = Default,
    family = binomial))
```
