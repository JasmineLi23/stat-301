{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e54c27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32c1b0a22b312747fee52943aa591977",
     "grade": false,
     "grade_id": "cell-dff56f438a8537a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Worksheet 11: Predictive versus generative modelling\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "By the end of this section, students will be able to:\n",
    "\n",
    "- Give examples of questions that can be answered by generative models and others that can be answered by predictive models.\n",
    "- Discuss how the research question being asked impacts the statistical modelling procedures.\n",
    "- Discuss why the model obtained directly from lasso is not the most suitable model for generative modelling and how post-lasso is one way to address this problem.\n",
    "- Write a computer script to perform post-lasso and use it to estimate a generative model.\n",
    "- Discuss post inference problems (e.g., double dipping into the data set) and current practical solutions available to address these (e.g., data-splitting techniques).\n",
    "- Write a computer script to apply currently available practical solutions to post inference problems.\n",
    "- Discuss how the research question being asked impacts the communication of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add787fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d5b7e569408a779c3c0cb6b4a51d522",
     "grade": false,
     "grade_id": "cell-42bb9496684954db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(glmnet)\n",
    "library(broom)\n",
    "library(leaps)\n",
    "source(\"tests_worksheet_11.r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f63caf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b8992ce9982474e952cedb72cc40657",
     "grade": false,
     "grade_id": "cell-c8e6e7e9d348a0fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2cfc7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52779145f143ab21bdac5c39db7e08ca",
     "grade": false,
     "grade_id": "cell-033bcebb28d726f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Statistical Modelling\n",
    "\n",
    "In statistical modelling, the objective is to capture how a response variable, $Y$, is associated with a set of input variables, $\\mathbf{X}=\\left(X_1, X_2, ..., X_p\\right)$. There are two main reasons that motivate us to model the relationship between $\\mathbf{X}$ and $Y$: (1) prediction; and (2) inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072aecf3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3493dcaaa720f2f67fccfbb99ae074be",
     "grade": false,
     "grade_id": "cell-7cbc7f723e3a6e0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Prediction\n",
    "\n",
    "Everyone is familiar with the usefulness of prediction. In fact, we are constantly making predictions on a daily basis. For example, \n",
    "You look at the weather outside and decide whether you should bring an umbrella with you or not. \n",
    "When you are driving and see a pedestrian approaching the street, you predict whether the pedestrian will walk in front of your car or wait. Many variables could affect the response here (i.e., whether the pedestrian will cross the street), such as, whether the pedestrian saw your car, if the pedestrian thinks you are going to stop on time, how late the pedestrian is to arrive at his/her destination, etc.\n",
    "When you have to go to an appointment, you predict how long it will take to arrive at your destination. Many variables could affect your response: Was there an accident in the way? Is there a construction site in the way? What time of the day is your appointment? \n",
    "These examples are very informal prediction processes. You make a judgment call based on your knowledge and past experiences.  Not very scientific, but still useful predictions! \n",
    "\n",
    "Statistical modelling, however, will allow you to model the relationship between the response variable and the covariates considered to be associated with it, based on data. Besides being less subjective and providing you with a methodologically sound way to learn about the relationship between $Y$ and $\\mathbf{X}$, statistical modelling has the crucial advantage of also giving you a measure of uncertainty. A few examples of questions questions that can be answered with prediction: \n",
    "\n",
    "- How much will be the selling price of a house with three bedrooms, 1300 sq. feet in Kitsilano? \n",
    "- Given that my blood albumin level is 44g/L, do I have a liver problem?\n",
    "- What will be my final grade in STAT 301, given that I'm studying 5 hours per week and got 85% in the midterm? \n",
    "- Is there a difference in final grades between two sessions of a course: one online and the other in-person? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2faec0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ae80927398bd4a138fe9c06b39f896c",
     "grade": false,
     "grade_id": "cell-2c79321fbf3f5ae6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Linear Regression as a Predictor\n",
    "\n",
    "- The conditional expectation is the best predictor of $Y$ given a vector of variables $\\boldsymbol{X}$: $E[Y|\\boldsymbol{X}]$\n",
    "\n",
    "\n",
    "- A LR assumes a linear form for this conditional expectation\n",
    "\n",
    "    - In general, this is only an *assumed* model, an *approximation* to the real form of $E[Y|\\boldsymbol{X}]$\n",
    "\n",
    "\n",
    "- If the conditional expectation is not linear, the LR can still be used as a predictor of $Y$ but it may not be the *best* one!\n",
    "    - For example, in DSCI100 you have used kNN to estimate the conditional expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1abb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8dbbc3253d2ccba0d338447b3d05c655",
     "grade": false,
     "grade_id": "cell-f82dbc9d8ca82a74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a80f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54eadc2384cbbb9e8ccba9509539a0b1",
     "grade": false,
     "grade_id": "cell-b915335777ccfac9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In some cases, we are interested in understanding the association between $Y$ and $\\mathbf{X}$. We want to know how $Y$ varies when $\\mathbf{X}$ changes. For example,\n",
    "\n",
    "- How does price affect the sales of iPhones?\n",
    "- What affects the price of a house more: the number of bedrooms or the number of bathrooms? \n",
    "- Is $\\text{CO}_2$ associated with temperature elevation? \n",
    "- Does the sex of the applicants influence the chance of admission at the University of British Columbia?\n",
    "- Has the social distancing measures influenced the spread of COVID-19 in Canada?\n",
    "\n",
    "In all the questions above, we are not primarily interested in making highly accurate predictions. Instead, we want to understand the relation between different variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2dedb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44012be21950de569fafa1f0ed7d44a2",
     "grade": false,
     "grade_id": "cell-d310f75b7d923b53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Naturally, in many cases, we are interested in both: inference and prediction. For example, one could be interested in answering \"How do social distancing measures influence the spread of COVID-19?\" but also in answering  \"How many cases can we expect to have if we implement some social distancing policies?\". Linear Models are an excellent initial approach in these cases, as they are highly interpretable and still perform reasonably well in many cases. \n",
    "\n",
    "As we move to more complex models, such as Neural Networks, we **might** obtain much higher prediction performance, but their interpretation is quite tricky, if possible at all.  Notwithstanding, if we are only interested in accurate predictions, these models can be extremely useful. For example, a bakery manager wants to know how many apple pies will be sold the next day in order to know how many to prepare today. In this case, it doesn't matter how they get the prediction, as long as the prediction is close. Otherwise, they miss sales if the forecast is too small; they lose money by throwing out too many pies if the forecast is too high. No matter how complex these models are, we can always estimate their prediction performance using strategies such as cross-validation. \n",
    "\n",
    "In other situations, however, our primary interest is in inference, i.e., in understanding the relationship between the response and the covariates. In these cases, we are willing to sacrifice some prediction performance for a more interpretable model that correctly depicts the variables' relationship. We are concerned about obtaining good estimates for the parameters of the models. \n",
    "\n",
    "<font color=\"blue\">**This week we will continue learning how to build and evaluate generative and predictive models.**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e6a50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d31d978cb06fb6180c15e0db8618862a",
     "grade": false,
     "grade_id": "cell-b6a517078dce99d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PART I: Model selection (cont.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848ffe4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e90bae61b8e3424693e42919ba05c6d",
     "grade": false,
     "grade_id": "cell-6f048d81f2cffe21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## In worksheet_08: \n",
    "\n",
    "### Select a model using *stepwise* algorithms\n",
    "\n",
    "- these are *greedy* algorithms \n",
    "\n",
    "- results depend on the order in which variables are selected \n",
    "\n",
    "- variables are either *in* (i.e., estimated coefficient different from zero) or *out* (i.e., estimated coefficient equal to zero)\n",
    "\n",
    "#### Instead, can we *smoothly* transition from a $0$ estimated coefficient to an estimated coefficient with a value different from $0$?\n",
    "\n",
    "Let's illustrate idea this with data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51c5a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fdf5371f4bc95b01c8984c47099b0d7",
     "grade": false,
     "grade_id": "cell-aa844e3860a3dcbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "In this section we will work with a richer real estate dataset, the [Ames `Housing` dataset](https://www.kaggle.com/c/home-data-for-ml-course/), compiled by Dean De Cock. It has 79 input variables on different characteristics of residential houses in Ames, Iowa, USA that can be used to predict the property's final price, `SalePrice`. We will use a subset of 21 continuous input variables:\n",
    "\n",
    "- `LotFrontage`: Linear $\\text{ft}$ of street connected to the house.\n",
    "- `LotArea`: Lot size in $\\text{ft}^2$.\n",
    "- `MasVnrArea`: Masonry veneer area in $\\text{ft}^2$.\n",
    "- `TotalBsmtSF`: Total $\\text{ft}^2$ of basement area.\n",
    "- `GrLivArea`: Above grade (ground) living area in $\\text{ft}^2$.\n",
    "- `BsmtFullBath`: Number of full bathrooms in basement.\n",
    "- `BsmtHalfBath`: Number of half bathrooms in basement.\n",
    "- `FullBath`: Number of full bathrooms above grade.\n",
    "- `HalfBath`: Number of half bathroom above grade.\n",
    "- `BedroomAbvGr`: Number of bedrooms above grade (it does not include basement bedrooms).\n",
    "- `KitchenAbvGr`: Number of kitchens above grade.\n",
    "- `Fireplaces`: Number of fireplaces.\n",
    "- `GarageArea`: Garage's area in $\\text{ft}^2$.\n",
    "- `WoodDeckSF`: Wood deck area in $\\text{ft}^2$.\n",
    "- `OpenPorchSF`: Open porch area in $\\text{ft}^2$.\n",
    "- `EnclosedPorch`: Enclosed porch area in $\\text{ft}^2$.\n",
    "- `ScreenPorch`: Screen porch area in $\\text{ft}^2$.\n",
    "- `PoolArea`: Pool area in $\\text{ft}^2$.\n",
    "\n",
    "The following variables will be used to construct a variable `ageSold`\n",
    "- `YearBuilt`: Original construction date.\n",
    "- `YrSold`: Year sold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a7b13b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "617c92c4ab25c5405f61960f761d0810",
     "grade": false,
     "grade_id": "cell-0e57653584382931",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run this code to prepare a working dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf0557",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5bc4cd0bdba8bdb20d5c340cddf9e43",
     "grade": false,
     "grade_id": "cell-39991d3415e985c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "library(broom)\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(faraway)\n",
    "library(mltools)\n",
    "library(leaps)\n",
    "library(glmnet)\n",
    "\n",
    "options(repr.plot.width=10, repr.plot.height=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3fa98",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc027062225fd32bd62f86e8e8f35fd2",
     "grade": false,
     "grade_id": "cell-0c438863066b229a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Housing <- read_csv(\"data/Housing.csv\")\n",
    "\n",
    "# Use `YearBuilt` and `YrSold` to create a variable `ageSold`\n",
    "Housing$ageSold <- Housing$YrSold - Housing$YearBuilt\n",
    "\n",
    "\n",
    "# Select subset of input variables\n",
    "Housing <- Housing %>%\n",
    "  select(\n",
    "    LotFrontage, LotArea, MasVnrArea, TotalBsmtSF, \n",
    "    GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, Fireplaces,\n",
    "    GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, ScreenPorch, PoolArea, ageSold, SalePrice\n",
    "  )\n",
    "\n",
    "\n",
    "# Remove those rows containing `NA`s and some outliers\n",
    "Housing <- drop_na(Housing)  %>% \n",
    "            filter(LotArea < 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dacb80",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50778d47dc408ec2620a1523af701d71",
     "grade": false,
     "grade_id": "cell-97eb861b2b2c3fb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "\n",
    "Housing$ID <- 1:nrow(Housing)\n",
    "training_Housing <- sample_n(Housing, size = nrow(Housing) * 0.60,\n",
    "  replace = FALSE\n",
    ")\n",
    "\n",
    "testing_Housing <- anti_join(Housing,\n",
    "  training_Housing,\n",
    "  by = \"ID\"\n",
    ")\n",
    "\n",
    "# I now remove the ID variable\n",
    "\n",
    "training_Housing <- training_Housing %>% select(-\"ID\")\n",
    "testing_Housing <- testing_Housing %>% select(-\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33beb2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f43f9f08e488bbee72e80d34495ce35",
     "grade": false,
     "grade_id": "cell-c32bcb07c61d87a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### In the following example, I use only the first 5 covariates to simplify the presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10f789",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ffc7f2c5ce78f937caded060bf93741",
     "grade": false,
     "grade_id": "cell-21841c7d8100c359",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run these cells to fit the \"forward\" selection algorithm on this (smaller) training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed445f5a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3c9f6a062b2ebd0caac6e55293d2ad2",
     "grade": false,
     "grade_id": "cell-670cc432afd163b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Housing_forward_sel <- regsubsets(\n",
    "  x = SalePrice ~ ., nvmax = 19,\n",
    "  data = training_Housing,#[,c(1:5,20)],\n",
    "  method = \"forward\",\n",
    ")\n",
    "\n",
    "coef(Housing_forward_sel,1) %>% round(3)\n",
    "coef(Housing_forward_sel,2) %>% round(3)\n",
    "coef(Housing_forward_sel,3) %>% round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf2e0a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5515cb8630300979bb3d963bcf3d02a",
     "grade": false,
     "grade_id": "cell-6b1297df2bdb2b7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The estimated coefficient for `ageSold` \"jumps\" from 0 to -1055.232. Similarly for other coefficients in other steps. \n",
    "\n",
    "### Can the selection be done more \"smoothly\"??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44aa211",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19dff8a2b7cfe8a61fc3a711c99eacb5",
     "grade": false,
     "grade_id": "cell-0d4c20409eb5f564",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Shrinkage (aka regularized or penalized) methods!!\n",
    "\n",
    "\n",
    "They shrink coefficients in a continuous way by adding a bound to their size!! \n",
    "\n",
    "$$\n",
    "\\ \\min_{\\beta_0, \\boldsymbol{\\beta}} \\ \\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\boldsymbol{X}_i \\boldsymbol{\\beta} \\right)^2\n",
    "$$\n",
    "#### subject to\n",
    "$$\n",
    "\\text{size of coefficients} \\le \\ C \\; \\text{ for some } C > 0.\n",
    "$$\n",
    "\n",
    "\n",
    "![](https://github.com/UBC-STAT/stat-301/blob/master/materials/worksheet_11/img/shrink.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe5db3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d296407f26534f042ca6145341e4356",
     "grade": false,
     "grade_id": "cell-ce0d8c6d7f77aac7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Mathematically\n",
    "\n",
    "<font color=\"blue\"> **Shrinkage methods minimize the residual sum of squares subject to a bound on the size of the coefficients (see picture)**\n",
    "    \n",
    "**Measuring size**\n",
    "\n",
    "We'll focus on 2 methods to measure the size of the coefficients (but there are many others that have been used and studied):\n",
    "\n",
    "\n",
    "- **Ridge** uses an $L_2-$norm to measure the size of the coefficients $$\\lVert \\beta \\rVert_2^2 = \\sum_{j = 1}^{p} \\beta_j^2.$$\n",
    "\n",
    "\n",
    "- **Lasso** uses an $L_1-$norm to measure the size of the coefficients $$\\lVert \\beta \\rVert_1 = \\sum_{j = 1}^{p} |\\beta_j|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab24be2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37b05a21e4ee9d70133b1a3d94047a7e",
     "grade": false,
     "grade_id": "cell-feca309f0f71333f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### LASSO: Least Absolute Shrinkage and Selection Operator\n",
    "\n",
    "- Proposed in 1996 by Tibshirani in \\emph{JRSS}\n",
    "\n",
    "- It *does* shrink coefficients to $0$, thus it can be used to *simultaneously select and train (estimate)* a model!!\n",
    "\n",
    "- It has been proposed to build strong predictive models\n",
    "\n",
    "**LASSO minimizes:**\n",
    "\n",
    "$$\n",
    "\\ \\min_{\\beta_0, \\boldsymbol{\\beta}} \\ \\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\boldsymbol{X}_i \\boldsymbol{\\beta} \\right)^2\n",
    "$$\n",
    "#### subject to\n",
    "$$\n",
    "\\sum_{j=1}^p \\, |\\beta_j| \\ \\le \\ C \\; \\text{ for some } C > 0.\n",
    "$$\n",
    "\n",
    "\n",
    "This is mathematically equivalent to minimizing a *penalized* RSS:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\boldsymbol{X}_i \\boldsymbol{\\beta} \\right)^2 + \\ \\lambda \\ \\sum_{j=1}^p \\, |\\beta_j| \\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf21ed6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df55bdcc519617487ea7d297cda40f91",
     "grade": false,
     "grade_id": "cell-b41f99ad599a85f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ridge\n",
    "\n",
    "- Proposed in 1970 by Hoerl, A.E. and Kennard, R. in \\emph{Technometrics}\n",
    "\n",
    "\n",
    "- It does not shrink parameters to $0$, so it does *not* select variables \n",
    "\n",
    "\n",
    "- It has been proposed as a method to address multicollinearity problems (we'll skip the math)\n",
    "\n",
    "**Ridge minimizes:**\n",
    "\n",
    "$$\n",
    "\\ \\min_{\\beta_0, \\boldsymbol{\\beta}} \\ \\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\boldsymbol{X}_i \\boldsymbol{\\beta} \\right)^2\n",
    "$$\n",
    "#### subject to\n",
    "$$\n",
    "\\sum_{j=1}^p \\, \\beta_j^2 \\ \\le \\ C \\; \\text{ for some } C > 0.\n",
    "$$\n",
    "\n",
    "\n",
    "This is mathematically equivalent to minimizing a *penalized* RSS:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\boldsymbol{X}_i \\boldsymbol{\\beta} \\right)^2 + \\ \\lambda \\ \\sum_{j=1}^p \\, \\beta_j^2 \\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a907ca7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "695aad146e0094d979a51c099ef71908",
     "grade": false,
     "grade_id": "cell-8d9fdc2d2ba753d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 How much to shrink?? the penalty parameter\n",
    "\n",
    "Mathematically, the problem is equivalent to minimizing a *penalized* RSS. For example, LASSO minimizes\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\boldsymbol{X}_i \\boldsymbol{\\beta} \\right)^2 + \\ \\lambda \\ \\sum_{j=1}^p \\, |\\beta_j| \\\n",
    "$$\n",
    "\n",
    "- The additional term $\\lambda \\sum_{j = 1}^{p} \\beta_j^2$ is the *penalty* term, and $\\lambda$ is called the *penalty parameter*. \n",
    "\n",
    "\n",
    "- If $\\lambda = 0$, this objective function is the same as that of LS!! Same estimators!! \n",
    "\n",
    "\n",
    "\n",
    "- As $\\lambda$ grows, coefficients are shrunk towards $0$ \n",
    "\n",
    "    - LASSO eventually shrinks them all to zero \n",
    "    - Ridge will never reach a value of zero \n",
    "    \n",
    "    \n",
    "- The penalty parameter $\\lambda$ can be selected using the data. This process is called \"tuning\".\n",
    "\n",
    "    - an option is to select the value that yields the smallest $\\text{MSE}_{\\text{test}}$. \n",
    "    - this tuning is done using an internal cross-validation or a validation set so that the model does not use *test* data\n",
    "    \n",
    "> This \"shrinkage\" process biases the estimated coefficients! We sacrifice bias for a lower variance to gain prediction performance!!\n",
    "\n",
    "**Important remark:** since the method depends on the *size* of the coefficients, we need to standardize the input variables (default option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ce48d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed05218eae4fd584390a138758255254",
     "grade": false,
     "grade_id": "cell-e49686eb6a9d692d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 In R:\n",
    "\n",
    "To train a regression using LASSO we can use the package `glmnet` in `R`\n",
    "\n",
    "For LASSO, we set `alpha = 1`. For Ridge, we set `alpha = 0`. \n",
    "\n",
    "> There are infinite other options in between known as *Elastic Net* \n",
    "\n",
    "1. `glmnet` requires: a matrix with input variables and a vector of responses\n",
    "\n",
    "\n",
    "2. we can find an \"optimal\" value of $\\lambda$ using the function `cv.glmnet()` \n",
    "\n",
    "> CV creates many *test sets* from the training set \n",
    "\n",
    "> we can define a **sequence**, grid, of values of $\\lambda$ to evaluate and choose from (optional)\n",
    "\n",
    "3. we can visualize how the estimated test-MSE changes for different values of $\\lambda$\n",
    "\n",
    "Run the code below to perform these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae3b02",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1f64e8f95bc2747c9646fe328932599",
     "grade": false,
     "grade_id": "cell-ec59602769ee4901",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Housing_X_train <- training_Housing %>% select(-\"SalePrice\")  %>% as.matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1c28b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7b9c83d89a6703a91ea274225c653f0",
     "grade": false,
     "grade_id": "cell-cb30d545c9eb1f6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Housing_X_train <- training_Housing %>% select(-\"SalePrice\")  %>% as.matrix()\n",
    "Housing_Y_train <- training_Housing %>% select(\"SalePrice\")  %>% as.matrix()\n",
    "\n",
    "\n",
    "Housing_X_test <- testing_Housing %>% select(-\"SalePrice\")  %>% as.matrix()\n",
    "Housing_Y_test <- testing_Housing %>% select(\"SalePrice\")  %>% as.matrix()\n",
    "\n",
    "Housing_cv_lambda_LASSO <- cv.glmnet(\n",
    "  x = Housing_X_train, y = Housing_Y_train,\n",
    "  alpha = 1,\n",
    "  lambda = exp(seq(5, 12, 0.1))\n",
    ")\n",
    "\n",
    "plot(Housing_cv_lambda_LASSO, main = \"Lambda selection by CV with LASSO\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4434e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab3e60fa72390ed127cd1161d1152412",
     "grade": false,
     "grade_id": "cell-47100083b8ddaf12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The plot shows the *estimated* test-MSE ($y$-axis) for a grid of values of $\\lambda$ ($x$-axis) on the natural log-scale. \n",
    "\n",
    "> the numbers at the top $x$-axis indicate the number of inputs whose estimated coefficients are different from zero for different values of $\\lambda$. \n",
    "\n",
    "> the error bars represent the variation across the different test sets of the CV (folds)\n",
    "\n",
    "The two vertical dotted lines correspond to two values of $\\lambda$:\n",
    "\n",
    "- $\\hat{\\lambda}_{\\text{min}}$ which provides the minimum MSE in the grid.\n",
    "\n",
    "\n",
    "- $\\hat{\\lambda}_{\\text{1SE}}$ largest value of lambda such that the corresponding MSE is within 1 standard error of that of the minimum (more penalization at a low cost)\n",
    "\n",
    "Run the code below to obtain $\\hat{\\lambda}_{\\text{min}}$ and call it `Housing_lambda_min_MSE_LASSO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8cefd9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d774381d525bbd6d1d41f05262b452a7",
     "grade": false,
     "grade_id": "cell-704aa1831061f9aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Housing_lambda_min_MSE_LASSO <- round(Housing_cv_lambda_LASSO$lambda.min, 4)\n",
    "\n",
    "round(Housing_lambda_min_MSE_LASSO,2)\n",
    "round(log(Housing_lambda_min_MSE_LASSO),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb287e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9a309c618e8591111706b15ba1ac978",
     "grade": false,
     "grade_id": "cell-35440dbccedadaf9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### LASSO \"smoothly\" selects variables and trains the corresponding model !\n",
    "\n",
    "> for values of lambda in the grid\n",
    "\n",
    "Run the code below to visualize the estimated regression coefficients over the $\\lambda$-grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a01e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a214139d08f5caa3cdc74624808cce99",
     "grade": false,
     "grade_id": "cell-a61cd4ea0e4d4941",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot(Housing_cv_lambda_LASSO$glmnet.fit, \"lambda\")\n",
    "abline(v = log(Housing_lambda_min_MSE_LASSO), col = \"red\", lwd = 3, lty = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0899f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b80f08151e9d6ef0ae1fd1037d30032c",
     "grade": false,
     "grade_id": "cell-778c34d899395d0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can compare the coefficients of the selcted model (for $\\hat{\\lambda}_{\\text{min}}$) with those of the full LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b66b4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50850f9280ebad1c91c8ffc3f446f253",
     "grade": false,
     "grade_id": "cell-96f63c55ee25a7ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "\n",
    "# Full LS\n",
    "\n",
    "Housing_full_OLS <- lm(SalePrice ~ .,\n",
    "data = training_Housing\n",
    ")\n",
    "\n",
    "# LASSO\n",
    "Housing_LASSO_min <- glmnet(\n",
    "  x = Housing_X_train, y = Housing_Y_train,\n",
    "  alpha = 1,\n",
    "  lambda = Housing_lambda_min_MSE_LASSO\n",
    ")\n",
    "\n",
    "Housing_LASSO_min.coef <- Housing_LASSO_min$beta\n",
    "\n",
    "Housing_reg_coef <- round(cbind(\n",
    "  Full_OLS = coef(Housing_full_OLS),\n",
    "  LASSO_min = c(\n",
    "    Housing_LASSO_min$a0,\n",
    "    as.vector(Housing_LASSO_min.coef)\n",
    "  )\n",
    "), 4)\n",
    "\n",
    "Housing_reg_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0cec3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8058c1d8edbd36eda1e2c76c1115b35",
     "grade": false,
     "grade_id": "cell-d827459db08ea397",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.4 Model selection and Prediction\n",
    "\n",
    "These models can be used to predict *new* data (out-of-sample)\n",
    "\n",
    "Many penalized estimators, including LASSO, have been proposed to build strong predictive models. Thus, they are tuned by minimizing a test-MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a73d4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cca9db5abbd286f65901e75b538ce5ea",
     "grade": false,
     "grade_id": "cell-d943850145f55831",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Full LS predictions\n",
    "\n",
    "Housing_test_pred_full_OLS <- predict(Housing_full_OLS, newdata = testing_Housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3984b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb0521903671083338a2d409f1096963",
     "grade": false,
     "grade_id": "cell-9256c46235f15452",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LASSO predictions\n",
    "\n",
    "Housing_test_pred_LASSO_min <- predict(Housing_LASSO_min,\n",
    "  newx = Housing_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c880f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33f68dfacf1c5cb464ccd9ef566adf52",
     "grade": false,
     "grade_id": "cell-28caaf4f782c09ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Comparison\n",
    "\n",
    "Housing_R_MSE_models <- rbind(tibble(\n",
    "  Model = \"OLS Full Regression\",\n",
    "  R_MSE = rmse(\n",
    "    preds = Housing_test_pred_full_OLS,\n",
    "    actuals = testing_Housing$SalePrice\n",
    "  )\n",
    "),\n",
    "    tibble(\n",
    "    Model = \"LASSO Regression with minimum MSE\",\n",
    "    R_MSE = rmse(\n",
    "      preds = Housing_test_pred_LASSO_min,\n",
    "      actuals = testing_Housing$SalePrice\n",
    "    )\n",
    "  )\n",
    ")\n",
    "\n",
    "Housing_R_MSE_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3487c12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7f4cecf7685326c595ad534ba89854f",
     "grade": false,
     "grade_id": "cell-dbc01d709db5154d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### In this example the full LS model has a better prediction performance than LASSO\n",
    "\n",
    "> results may depend on the split of the data. We need to make several splits to assess these models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9205d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcc6cfe53160435fcde1555da222f5ab",
     "grade": false,
     "grade_id": "cell-12da77d8ca63df55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 Conclusions\n",
    "\n",
    "- Shrinkage methods can be used to select variables to build a model depending on the penalty function used\n",
    "\n",
    "- Shrinkage methods shrink coefficients in a continuous way by adding a bound to their size!!\n",
    "\n",
    "- In general, they are tuned to build predictive models. Penalty levels are chosen so that the CV-MSE is minimized (or low enough)\n",
    "\n",
    "- LASSO is a well known example that penalized the RSS with an $L_1$ penalty\n",
    "\n",
    "    - coefficients can be shrunk to 0\n",
    "    \n",
    "- Ridge is a well known example that penalized the RSS with an $L_2$ penalty\n",
    "\n",
    "    - coefficients won't be shrunk to 0\n",
    "    \n",
    "    - it was first proposed to address problems of multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40216a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6e61fef4f5b86535b6061e26a5929bb",
     "grade": false,
     "grade_id": "cell-4db8971f16f54c5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## PART II: Model Selection and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6d03e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e6a9e5ff270d74a8a5012a6aa7898e1",
     "grade": false,
     "grade_id": "cell-52b0432e6c14a90f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are many aspects involved in selecting a model that goes beyond variable selection, for example,\n",
    "\n",
    "- Do we want a parametric or non-parametric approach? \n",
    "- Do we want to assume a functional form for the relationship between $Y$ and $\\mathbf{X}$ (e.g., linear, quadratic, exponential, logarithmic)?\n",
    "- Prediction performance. \n",
    "- Is model interpretability important?\n",
    "\n",
    "In previous lectures, we have discussed different ways of selecting models for estimation problems in which the interpretability of the model is crucial, as well as for prediction problems.\n",
    "\n",
    "In particular, we have learned many ways of comparing: (1) $R^2$, (2) adjusted $R^2$, (3) BIC, (4) Cp, among others.\n",
    "\n",
    "We also explored different techniques to select a desired model:\n",
    "\n",
    "- F-test\n",
    "\n",
    "- Stepwise Algorithms (e.g., Forward Selection, Backward Selection)\n",
    "\n",
    "- Penalized methods (e.g. LASSO)\n",
    "\n",
    "#### Can we still make inference using the selected models??\n",
    "\n",
    "In this course, we learned how to make inference (e.g, calculate confidence interval and hypotheses tests) for a fixed model. However, when we apply any of these model selection methods, we are searching for the combination of variables that will give us the best model (according to a given metric). So the variables in our final models are not fixed; instead, they are selected adaptively based on **the data at hand**. \n",
    "\n",
    "Two questions arise then: \n",
    "\n",
    "1. Do these model selection algorithms affect the inference about the parameters of the model? \n",
    "\n",
    "2. Is the way we interpret the models still the same? \n",
    "\n",
    "Now we will investigate the first question. For now, let's focus on the forward selection. \n",
    "\n",
    "#### Forward Selection Review\n",
    "Suppose we have $p$ covariates $X_1, X_2,\\ldots,X_p$ to explain our response $Y$. The full model is given by:\n",
    "\n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1 X_{i1} + \\ldots + \\beta_p X_{ip} + \\varepsilon\n",
    "$$\n",
    "\n",
    "We want to find the best subset of variables to explain $Y$. Searching the best subset using brute force would require us to fit prohibitive number of models to compare (see table below).\n",
    "\n",
    "| number of covariates (*p*) | Number of possible models |\n",
    "| ---------------------------|-------------------:|\n",
    "| 10 | 1,024 |\n",
    "| 20 | 1,048,576 |\n",
    "| 30 | 1,073,741,824 |\n",
    "\n",
    "\n",
    "The forward selection strategy helps us to find good models among the insane numbers of models shown in the table above. But, unfortunately, it is not guaranteed to find the best model (or even good models). \n",
    "\n",
    "It starts with the **null model** (i.e., a model with no covariates, only the intercept $\\beta_0$):\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\beta_0 + \\epsilon\n",
    "$$\n",
    "\n",
    "Then, among the remaining variables, it searches for the one that improves the model the most and incorporates the variable into to the model. It keeps incorporating one variable at a time until there's no variable left that would improve the model. \n",
    "\n",
    "But what do we mean by \"improves the model the most?\" - There are different criteria one can use to \"measure\" this. Common choices are $C_p$, *AIC*, *BIC*, *F-test*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a3b93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38808f1e0ad923609c3391d9cc5fda7c",
     "grade": false,
     "grade_id": "cell-aa995988a3e3146f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.1**<br>\n",
    "\n",
    "In this exercise, we will explore if the forward selection strategy affects the model inference. For this purpose, we are going to use simulation in order for us to know the truth. Here's what we are going to do: \n",
    "\n",
    "1. We are going to consider a response variable $Y$ and $p=10$ covariates. However, none of the covariates will have any effect on $Y$; they are all independent. (we already know the truth)\n",
    "\n",
    "\n",
    "2. Generate 100 observations of each variable from a normal distribution.\n",
    "\n",
    "\n",
    "3. Apply only the first step of forward selection. In other words, we are looking to add the first variable only among ten potential candidates.\n",
    "    - The metric we are going to use is the F statistic.\n",
    "\n",
    "\n",
    "4. `replicate` this study 1,000 times and measure the errors.\n",
    "\n",
    "We have already simulated the data for you (Steps 1 and 2 above) in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72d705",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9deb8d30aaef06cc83299f1c0b2e94ed",
     "grade": false,
     "grade_id": "cell-8196a2c8170de40f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell before continuing \n",
    "set.seed(20211113)\n",
    "\n",
    "n <- 100    # sample size\n",
    "p <- 10     # number of variables\n",
    "rep <- 1000 # number of replications\n",
    "\n",
    "means <- runif((p+1), 3, 10) # the mean that will be used in the \n",
    "                             # Normal distribution for simulation.\n",
    "                             # The +1 refers to Y.  \n",
    "\n",
    "dataset <- as_tibble(\n",
    "  data.frame(\n",
    "    matrix(\n",
    "      round(rnorm((p + 1) * n * rep, \n",
    "            means, 10), 2), \n",
    "      ncol = p+1, \n",
    "      byrow = TRUE\n",
    "    )\n",
    "  ) %>% \n",
    "  rename(Y = X11) %>% \n",
    "  mutate(replicate = rep(1:rep, n)) %>% \n",
    "  arrange(replicate) \n",
    ")\n",
    "\n",
    "head(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc40c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa905db5ae615b6e39561b799cfa7089",
     "grade": false,
     "grade_id": "cell-038f4a539d529c3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.1.1 - Warm up**<br>\n",
    "{points: 1}\n",
    "\n",
    "To help you visualize the code abstraction, let's do a more intuitive exercise. \n",
    "Using the `dataset` tibble, fit one `lm` for each replicate using all 10 covariates to explain $Y$. Store the `lm` models in a column named `models`.\n",
    "\n",
    "_Assign your data frame to an object called `full_models`. Your data frame should have three columns: `replicate`, `data`, and `models`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534afbfd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86766ab99f82c294f0ccf2092e6a80cc",
     "grade": false,
     "grade_id": "cell-d5570a80d2c30190",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# full_models <- \n",
    "#     ... %>% \n",
    "#     group_by(...) %>% \n",
    "#     nest() %>% \n",
    "#     mutate(models = map(...))\n",
    "\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "# Try exploring the columns of your data frame. \n",
    "# Check full_models$data[[1]] and full_models$models[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e65c4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c42a89305c01ee46946d12be7a899b3",
     "grade": true,
     "grade_id": "cell-a4f7c92013fb0e66",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.1.1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12db00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "545e9f9fd2f5926792c209a0832f0a2f",
     "grade": false,
     "grade_id": "cell-7f2f7877c77c43b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To help speed things up, we created a function for you that receives a data frame, performs the first forward selection step, and returns the F-statistic. What is the F-statistic again? In this case, the F-statistic measures if the Residuals Sum of Squares when incorporating a covariate into the model decreases significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfa958",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6aa1992d23fde7e093731064b758a350",
     "grade": false,
     "grade_id": "cell-f182e694b49e309e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "forward_selection_step1 <- function(dataset){\n",
    "    #' Returns the F-statistic of the first\n",
    "    #' step of forward selection.\n",
    "    #'\n",
    "    #' @param dataset the dataset to be used\n",
    "\n",
    "    selected_model <- lm(Y ~ ., data = dataset[,c(paste(\"X\",1, sep = \"\"), \"Y\")])\n",
    "    F_selected <- glance(selected_model) %>% pull(statistic)\n",
    "    \n",
    "    for( j in 2:(ncol(dataset)-1) ){ # fits one lm for each covariate and calculate the F statistic \n",
    "        model <- lm(Y ~ ., data = dataset[,c(paste(\"X\",j, sep = \"\"), \"Y\")])\n",
    "        F <- glance(model) %>% pull(statistic)\n",
    "        \n",
    "        \n",
    "        if (F > F_selected){\n",
    "            F_selected <- F\n",
    "            selected_model <- model\n",
    "        }\n",
    "    }\n",
    "    return(selected_model)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8eee59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00cdc9ca6c4165fa707965c7ad2c1194",
     "grade": false,
     "grade_id": "cell-e2ca8b6be7d1c675",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.1.2**<br>\n",
    "{points: 1}\n",
    "\n",
    "Using the `dataset` tibble, calculate the F-statistic of the model selected in the first step of forward selection. Store the model in a column named `fs_model`. Then, extract the F-statistic from the model and store it in a column named `F`.\n",
    "\n",
    "_Assign your data frame to an object called `forward_selection_F`. Your data frame should have four columns: `replicate`, `data`, `fs_model`, and `F`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a70bba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0e4ec0aebeac6c3f96540039e3e9075",
     "grade": false,
     "grade_id": "cell-bf37cd17c298bbb8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# forward_selection_F <- \n",
    "#     ... %>% \n",
    "#     group_by(...) %>% \n",
    "#     nest() %>% \n",
    "#     mutate(\n",
    "#         ... = map(...), \n",
    "#         ... = ..._dbl(...)\n",
    "#     )\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "head(forward_selection_F, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e605b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7fda797c9850aeea154a80d5d444a95",
     "grade": true,
     "grade_id": "cell-eabe9532c5e44b6d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.1.2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294771c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c57e629c83f84a8678996a9e67a14db0",
     "grade": false,
     "grade_id": "cell-baa571c624ad81d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.1.3** \n",
    "<br> {points: 1}\n",
    "\n",
    "Suppose we want to test, at 5% significance, whether the decrease in the RSS was significant by adding the variable chosen by the forward selection strategy. In this case, $\\text{F-statistic}\\sim F_{1,98}$. \n",
    "\n",
    "\n",
    "What value should we compare the F-statistic against? \n",
    "\n",
    "\n",
    "_Assign your answer to an object called `F_critical`. Your answer should be a single number._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfc8c2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01d75da470f66dd4af3e5e3ed30b828e",
     "grade": false,
     "grade_id": "cell-63ecf21e457d958b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# F_critical <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "F_critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5bcc50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42facfefa4f7d5396dc887e19df8ad44",
     "grade": true,
     "grade_id": "cell-89cbd2cfdcf97c67",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.1.3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec61e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "195fbf3df8a8da69e5673a14e4ea4bb2",
     "grade": false,
     "grade_id": "cell-c4f15d48f178784a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.1.4** \n",
    "<br> {points: 1}\n",
    "\n",
    "Knowing that none of our covariates are relevant to model $Y$, if we use the `F_critical` you calculated in the previous questions, what proportions of replications would you expect to wrongly reject the null hypothesis that the variable is not significant?\n",
    "\n",
    "_Assign your answer to an object called `nominal_type_I_error`. Your answer should be a single number._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259555f8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "295db2ccd2742ff13fb194a4fe1be60d",
     "grade": false,
     "grade_id": "cell-beba772a72681900",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# nominal_type_I_error <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "nominal_type_I_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013daa6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6e07a17bac97c21ad5122a48c4456f6",
     "grade": true,
     "grade_id": "cell-8d98c976a828e040",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.1.4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc7c7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "697b4ebd8b9832c7b349fa473df1178a",
     "grade": false,
     "grade_id": "cell-ccc6a385e1043ed0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.1.5** \n",
    "<br> {points: 1}\n",
    "\n",
    "Check the proportions of F-statistics in the `forward_selection_F` tibble that are above the `F_critical` you calculated. \n",
    "\n",
    "_Assign your answer to an object called `forward_selection_type_I_error`. Your answer should be a single number._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642bea9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea1b7e1c8fc3c1a61ec74d6643c32d32",
     "grade": false,
     "grade_id": "cell-8c4e6b14508b40ee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# forward_selection_type_I_error <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "forward_selection_type_I_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6bf221",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "257f9eb3aa38e1828a2d59338f59da18",
     "grade": true,
     "grade_id": "cell-6c32bbee9d728156",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.1.5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2121a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e862e91d5bed6ccf90ce0362edd1d5a",
     "grade": false,
     "grade_id": "cell-fb759982412336e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Class discussion \n",
    "Contrast the `forward_selection_type_I_error` and `nominal_type_I_error`. Are they similar? Why do you think this is happening. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad366c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2254b90c224b58f2c0083ce1ec75be7",
     "grade": false,
     "grade_id": "cell-d34cab1fa0787324",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.1 The double use of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775dd9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb4e04c86df282b0c5bd8a423923ad12",
     "grade": false,
     "grade_id": "cell-e7302f3b294124f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "But the Type I Error after the forward selection was significantly higher than the nominal level of 5%? Well, if we are looking for the most relevant covariates in a dataset, it is not surprising that we frequently find these covariates significant. In our case above, the *F-statistic* compares the reduction of SSR after adding a variable. But this was not what we did. Instead, we deliberately searched among the variables for the one that had the highest F-statistic. In this sense, we are not testing the inclusion of **a** variable; we are testing the inclusion of  **the** variable that yields the largest F-statistic **in the sample at hand**. Hence, we have a much higher chance of wrongly rejecting $H_0$.  \n",
    "\n",
    "Ok, we identified the problem: we are using the same sample to find the variable that yields the largest test statistic **in the sample we have**. \n",
    "\n",
    "But what if we split the dataset into two parts, one used for model selection and the other used for inference (similarly to what we've done in cross-validation)? Would that solve the problem? Let's investigate! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043329f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4567660588ac376a641a050e173b0e80",
     "grade": false,
     "grade_id": "cell-1a771305759350ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.2** \n",
    "<br> {points: 1}\n",
    "\n",
    "In this exercise we are again going to use the tibble `dataset`. But this time we are going to split our dataset into two parts. One part we are going to use for inference, and the other part we are \n",
    "going to use for inference. For this exercise, let's split the dataset in half. \n",
    "\n",
    "Here's what you need to do: \n",
    "\n",
    "1. Shuffle the dataset, so we know that the observations are in random order. \n",
    "\n",
    "2. Using the first 50 observations, apply the first step of forward selection; store the selected model in `fs_model` column. Also, extracts the F-statistic of the `fs_model` and stores it in a column called `F_fs`.\n",
    "\n",
    "3. Fit the model selected in Step 2 using the 50 remaining observations and save it in a column named `inference_model`. Also, extracts the F-statistic of the `inference_model` and stores it in a column called `F_inference`. \n",
    "\n",
    "_Assign your data frame to an object called `fs_error_split`. Your data frame should have 6 columns: `replicate`, `data`, `fs_model`, `F_fs`, `inference_model`, `F_inference`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e70736",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "172f555d1f403e4014ff5defb9f78c02",
     "grade": false,
     "grade_id": "cell-b544356ca3740453",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "set.seed(20211113) # Do not change this.\n",
    "\n",
    "# fs_error_split <- \n",
    "#     ... %>% \n",
    "#     sample_n(...) %>%\n",
    "#     ... %>% \n",
    "#     ... %>% \n",
    "#     mutate(\n",
    "#         fs_model = ...(..., .f = function(d) forward_selection_step1(d %>% head(50))), \n",
    "#         F_fs = ...,\n",
    "#         inference_model = map2(.x = ..., .y = ..., ~ update(.y, .~., data = .x %>% tail(50))), \n",
    "#         F_inference =  ...)\n",
    "#     )\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "        \n",
    "head(fs_error_split) %>% \n",
    "    select(F_fs, F_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e35bbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a1631585b85d7d6bf68abf9e70b7d92",
     "grade": true,
     "grade_id": "cell-87cd37c4793c4935",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce156766",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6f155cfee2a9aace35e301c68599e48",
     "grade": false,
     "grade_id": "cell-d98b56dbc789635d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.3** \n",
    "<br> {points: 1}\n",
    "\n",
    "Check the proportions of F-statistics in the `F_inference` column that are above the `F_critical` you calculated. (Hint: in this case $\\text{F-statistic}\\sim F_{1,48}$. \n",
    "\n",
    "_Assign your answer to an object called `fs_split_type_I_error`. Your answer should be a single number._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ac98d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9318c6b949d2dab70f8c2631fd51b18",
     "grade": false,
     "grade_id": "cell-d63bd5b3efb2ae77",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# fs_split_type_I_error <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "fs_split_type_I_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7e65d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f5b9a87fc66c993dac72f88c448c4dc",
     "grade": true,
     "grade_id": "cell-9b2666ff7cd4089f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e615d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d123b2ac89ddfdfbba5970be0e3d300d",
     "grade": false,
     "grade_id": "cell-851de6eae8846cad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.4**\n",
    "<br>{points: 1}\n",
    "\n",
    "True or false?\n",
    "\n",
    "If split the data into model selection and inference split, the type I error of the F-test after the forward selection is close to the significance level. \n",
    "\n",
    "_Assign your answer to an object called `answer2.4`. Your answer should be either \"true\" or \"false\", surrounded by quotes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316403a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4057c1b2ec1ee4b05a8a056df7dcafce",
     "grade": false,
     "grade_id": "cell-f1a4e23880603605",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer2.4 <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4f30c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81c0139ee7a615a61cf739b7726dc203",
     "grade": true,
     "grade_id": "cell-b588d3c3d475102a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69469f9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a25ba72b3bd3f0f47e2d1547d573e9b9",
     "grade": false,
     "grade_id": "cell-219ec4f75a9ad8f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2 Lasso - two problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145e2dd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ab94585c9f1245f7ad200fcaf0c443",
     "grade": false,
     "grade_id": "cell-05afee85a589325b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The problem we discussed with the forward selection *also happens with Lasso*. Similarly, we can address it by splitting the dataset -- one part for selecting variables using lasso and the other for fitting the model for inference. \n",
    "\n",
    "However, there's another problem with Lasso for inference. Lasso's estimators are biased. Bias estimators are estimators whose sampling distributions are not centred on the true value of the parameter. \n",
    "\n",
    "To study this problem, we are going to use a new simulation, Our population is generated below and it is store in `lasso_sim` tibble.\n",
    "\n",
    "The population was generated such that:\n",
    "\n",
    "$$\n",
    "E[Y|X_1, X_2] = 75X_1 - 5 X_2 + 0 X_3\n",
    "$$ \n",
    "\n",
    "Therefore, the true parameters are $\\beta_1=75$ and $\\beta_2=-5$. Note that $\\beta_3$ is not a relevant variable and, hopefully, Lasso will remove it from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8fa6c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6e5a079e9636b0aa248c6f98b4692ee",
     "grade": false,
     "grade_id": "cell-95df2b5999161495",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell before continuing \n",
    "\n",
    "set.seed(20211113) # Do not change this.\n",
    "\n",
    "n <- 1000    # sample size\n",
    "rep <- 1000 # number of replications\n",
    "\n",
    "lasso_sim <- \n",
    "    tibble(\n",
    "        X1 = round(\n",
    "                rnorm(n * rep, 0, 10), \n",
    "                2),\n",
    "        X2 = round(\n",
    "                rnorm(n * rep, 0, 10), \n",
    "                2),\n",
    "        X3 = round(\n",
    "                rnorm(n * rep, 0, 20), \n",
    "                2),\n",
    "        Y = round(75 * X1 - 5*X2 + rnorm(n * rep, 0, 400),2)) %>% \n",
    "    mutate(replicate = rep(1:rep, n)) %>% \n",
    "    arrange(replicate) \n",
    "\n",
    "\n",
    "head(lasso_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e90e4ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da635f4eed09dbff7944155a19f405cc",
     "grade": false,
     "grade_id": "cell-4665f4f34800f47c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.5**<br>\n",
    "{points: 1}\n",
    "\n",
    "Using the `lasso_sim` tibble, fit a lasso model for each replicate using $\\lambda=30$. Store the models in a column named `lasso_models`. \n",
    "\n",
    "_Assign your data frame to an object called `lasso_study`. Your data frame should have four columns: `replicate`, `data`, and `lasso_model`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18767685",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9480522c348ae66766940290da644e8e",
     "grade": false,
     "grade_id": "cell-aa46e5df61e9558b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# lasso_study <- \n",
    "#     ... %>% \n",
    "#     ... %>% \n",
    "#     ... %>% \n",
    "#     mutate(\n",
    "#         lasso_model = ...(...,\n",
    "#                           ~...(.x %>% select(-Y) %>% as.matrix(), \n",
    "#                                   .x %>% select(Y) %>% as.matrix(), \n",
    "#                                   alpha = ..., \n",
    "#                                   lambda = ...)))\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "lasso_study %>% head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86648684",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00f23c9c9688fa0f10b86d6f20588a43",
     "grade": true,
     "grade_id": "cell-d68d670ab6c2496c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f7f81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "614206049dfcc3e7c36b6ff57d818c0c",
     "grade": false,
     "grade_id": "cell-2a61e4f25256cc82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.6**<br>\n",
    "{points: 1}\n",
    "\n",
    "Extract the coefficient for `beta_1` from each `lasso_model` in the `lasso_study` tibble. Store the coefficent in a column name `lasso_beta1` in the same `lasso_study` tibble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3cfd4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7367c35808fe782594e241e6417f8fad",
     "grade": false,
     "grade_id": "cell-eb3b9bba09b060d0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# lasso_study <- \n",
    "#     ... %>% \n",
    "#     ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "lasso_study %>% \n",
    "    select(-data) %>% \n",
    "    head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26531f67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7129320ddccf3ce687d609c822d2a699",
     "grade": true,
     "grade_id": "cell-8ebd4cdf24d174b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef540c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41c482ed40e9068f559c7df90085fb90",
     "grade": false,
     "grade_id": "cell-3472a426b731a695",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.7**\n",
    "<br> {points: 1}\n",
    "\n",
    "Plot the sampling distribution of $\\hat{\\beta}_1$ obtained by Lasso.\n",
    "\n",
    "\n",
    "_Assign your plot to an object called `lasso_beta1_sampling_dist`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac61836",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5779b29e5d3766e625ebfffe7c02f9ca",
     "grade": false,
     "grade_id": "cell-21508cb2d3c1b17b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# lasso_beta1_sampling_dist <- \n",
    "#     lasso_study %>% \n",
    "#     ggplot() + \n",
    "#     geom_...(aes(...), color='white') +\n",
    "#     geom_vline(xintercept = 75, color = 'red') + \n",
    "#     geom_text(aes(75, 110), label = \"True Value of the Parameter\", color = 'red') \n",
    "#     geom_text(aes(75, 80), label = \"True Value of\\n the Parameter\", color = 'red', size = 7) +\n",
    "#     theme(text = element_text(size = 18))\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "lasso_beta1_sampling_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a92fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa4a61091e05aed301997506f7b0ea3a",
     "grade": true,
     "grade_id": "cell-3b289632e05ad195",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.7()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bc4e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "babe9fb5a50050e0220840077af29b45",
     "grade": false,
     "grade_id": "cell-ae58776784170e6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.8**\n",
    "<br>{points: 1}\n",
    "\n",
    "True or false?\n",
    "\n",
    "The sampling distribution of the lasso estimator of $\\beta_1$ is centered around the true $\\beta_1$.\n",
    "\n",
    "_Assign your answer to an object called `answer2.8`. Your answer should be either \"true\" or \"false\", surrounded by quotes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707ad82",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1404b5825492a3ed3a45290b9e67db6d",
     "grade": false,
     "grade_id": "cell-cbc53e1b3025df42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer2.8 <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fe597",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc2392ee9350c495adb4f535ce1ccaee",
     "grade": true,
     "grade_id": "cell-2c1d0b19fb80ea91",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.8()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf344ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1021157c8a08afc1295e51890edd7f9",
     "grade": false,
     "grade_id": "cell-70b6424c44d2cb6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.9**<br>\n",
    "{points: 1}\n",
    "\n",
    "Do deal with the Lasso's bias problem, we can use the variables selected by Lasso, but not the coefficients. Instead, we make a second fit using regular least squares. In the cell below we have done it for you. Here's what we did:\n",
    "\n",
    "1. Add a new column to `lasso_study` tibble, named `lasso_selected_covariates` with the covariates selected by Lasso (i.e., with coefficients different from 0).\n",
    "\n",
    "\n",
    "2. We fitted a linear model using `lm` (regular least square) and only the `lasso_selected_covariates`.\n",
    "\n",
    "\n",
    "3. We extracted $\\beta_1$ from the `lm` model, and saved it on a column called `ls_beta1`.\n",
    "\n",
    "Your job is to plot the sampling distribution of $\\hat{\\beta}_1$ obtained by the regular least square, using the variables selected by Lasso.\n",
    "\n",
    "_Assign your plot to an object called `post_lasso_lm_beta1_sampling_dist`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e8a34",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e7151c2d9aeb829a0c8eff35b7338b7",
     "grade": false,
     "grade_id": "cell-6920e00dbec79884",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell before continuing\n",
    "\n",
    "lasso_study <- \n",
    "    lasso_study %>% \n",
    "    mutate(\n",
    "        lasso_selected_covariates = map(.x = lasso_model, \n",
    "                                        ~as_tibble(\n",
    "                                                as.matrix(coef(.x)),\n",
    "                                                rownames='covariate') %>%\n",
    "                                                filter(covariate != '(Intercept)' & abs(s0) > 10e-6) %>% \n",
    "                                                pull(covariate)),\n",
    "        ls_fit = map2(.x = data, .y = lasso_selected_covariates,\n",
    "                     ~lm(Y ~ ., data = .x[,c(.y, 'Y')])),\n",
    "        ls_beta1 = map_dbl(.x = ls_fit, ~tidy(.x) %>% filter(term == 'X1') %>% pull(estimate)))\n",
    "\n",
    "\n",
    "lasso_study %>% \n",
    "    select(-data, -lasso_model, -ls_fit) %>% \n",
    "    head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1707c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2db9c9581d5024293846a809b4343987",
     "grade": false,
     "grade_id": "cell-dd0e4b4d53066426",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# post_lasso_lm_beta1_sampling_dist <- \n",
    "#     lasso_study %>% \n",
    "#     ggplot() + \n",
    "#     geom_...(aes(...), color='white') +\n",
    "#     geom_vline(xintercept = 75, color = 'red') + \n",
    "#     geom_text(aes(75, 80), label = \"True Value of\\n the Parameter\", color = 'red', size = 7) +\n",
    "#     theme(text = element_text(size = 18))\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "post_lasso_lm_beta1_sampling_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b095d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "585f3816ef9fd17a42b3b47ff13865da",
     "grade": true,
     "grade_id": "cell-69a5a5d6f0125c49",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.9()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7c05b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ab81a05c67edb3c890426b18f4e7623",
     "grade": false,
     "grade_id": "cell-47eca90d66532852",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.3 Conclusions\n",
    "\n",
    "#### Post-inference problem:\n",
    "\n",
    "- we can not use the same data to select variables of the model and to conduct inference (\"doble dipping\"). \n",
    "\n",
    "- the inference results given by the `lm` are not valid (as seen in the first part of the worksheet). \n",
    "\n",
    "- if we split the data, we can use one part to select and the other part to estimate and build tests.\n",
    "\n",
    "- more sophisticated methods have been proposed to address this problem (beyond the scope of this course).\n",
    "\n",
    "\n",
    "#### Lasso has two problems:\n",
    "\n",
    "- **Biased estimators**: we can take care of this by fitting regular least squares on the variables selected by Lasso. This approach is called **post-lasso**.\n",
    "\n",
    "- **Post-inference**: fitting a LS regression after Lasso, we are using the data to select the variables as well as to conduct inference. We cannot rely on the inference given by the `lm`, unless we split the data to take care of this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
